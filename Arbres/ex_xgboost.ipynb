{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo: 100.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Importar las bibliotecas necesarias\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Cargar el conjunto de datos (usaremos el conjunto de datos Iris como ejemplo)\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear el objeto DMatrix para XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Definir los parámetros del modelo\n",
    "params = {\n",
    "    'max_depth': 3,  # Profundidad máxima del árbol\n",
    "    'eta': 0.1,  # Tasa de aprendizaje\n",
    "    'objective': 'multi:softmax',  # Objetivo: clasificación multiclase\n",
    "    'num_class': 3  # Número de clases en el conjunto de datos\n",
    "}\n",
    "\n",
    "# Entrenar el modelo\n",
    "num_round = 100  # Número de iteraciones o árboles a construir\n",
    "model = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "preds = model.predict(dtest)\n",
    "\n",
    "# Calcular la precisión del modelo\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "print(f'Precisión del modelo: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Cargar el conjunto de datos Iris (o cualquier otro conjunto de datos)\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear un clasificador CatBoost\n",
    "model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, loss_function='MultiClass')\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=False)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "# Calcular la precisión del modelo\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "print(f'Precisión del modelo: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 22:07:12.001025: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-13 22:07:12.016318: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-13 22:07:12.045603: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-13 22:07:12.045629: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-13 22:07:12.047427: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-13 22:07:12.053917: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-13 22:07:12.054458: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-13 22:07:12.574961: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 11s 11ms/step - loss: 0.1746 - accuracy: 0.9471 - val_loss: 0.0515 - val_accuracy: 0.9843\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 11s 11ms/step - loss: 0.0471 - accuracy: 0.9850 - val_loss: 0.0379 - val_accuracy: 0.9880\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 11s 11ms/step - loss: 0.0354 - accuracy: 0.9894 - val_loss: 0.0264 - val_accuracy: 0.9915\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 14s 15ms/step - loss: 0.0279 - accuracy: 0.9908 - val_loss: 0.0337 - val_accuracy: 0.9898\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0216 - accuracy: 0.9931 - val_loss: 0.0344 - val_accuracy: 0.9898\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0344 - accuracy: 0.9898\n",
      "Precisión en el conjunto de prueba: 0.989799976348877\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Cargar datos de entrenamiento y prueba de MNIST\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Preprocesamiento de datos\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "# Codificación one-hot de las etiquetas\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels)\n",
    "\n",
    "# Crear el modelo CNN\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_data=(test_images, test_labels))\n",
    "\n",
    "# Evaluar la precisión del modelo en el conjunto de prueba\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Precisión en el conjunto de prueba:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 1s 0us/step\n",
      "25000 secuencias de entrenamiento\n",
      "25000 secuencias de prueba\n",
      "Pad sequences (muestras x longitud)\n",
      "input_train shape: (25000, 500)\n",
      "input_test shape: (25000, 500)\n",
      "Entrenando...\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.5388 - acc: 0.7120 - val_loss: 0.3752 - val_acc: 0.8366\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 29s 46ms/step - loss: 0.3730 - acc: 0.8454 - val_loss: 0.3423 - val_acc: 0.8512\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 37s 59ms/step - loss: 0.2941 - acc: 0.8825 - val_loss: 0.3816 - val_acc: 0.8304\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 35s 56ms/step - loss: 0.2785 - acc: 0.8950 - val_loss: 0.3407 - val_acc: 0.8568\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 35s 56ms/step - loss: 0.2293 - acc: 0.9145 - val_loss: 0.4000 - val_acc: 0.8540\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 35s 56ms/step - loss: 0.1980 - acc: 0.9278 - val_loss: 0.3579 - val_acc: 0.8668\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 33s 53ms/step - loss: 0.1701 - acc: 0.9387 - val_loss: 0.4315 - val_acc: 0.8588\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 38s 60ms/step - loss: 0.1550 - acc: 0.9448 - val_loss: 0.4335 - val_acc: 0.8612\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 35s 57ms/step - loss: 0.1194 - acc: 0.9578 - val_loss: 0.4480 - val_acc: 0.8526\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 36s 57ms/step - loss: 0.0975 - acc: 0.9660 - val_loss: 0.5019 - val_acc: 0.8374\n",
      "Evaluando...\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.5253 - acc: 0.8299\n",
      "Precisión en el conjunto de prueba: 0.8299199938774109\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "# Cargar la base de datos IMDb\n",
    "max_features = 10000\n",
    "maxlen = 500\n",
    "batch_size = 32\n",
    "\n",
    "print('Cargando datos...')\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "print(len(input_train), 'secuencias de entrenamiento')\n",
    "print(len(input_test), 'secuencias de prueba')\n",
    "\n",
    "print('Pad sequences (muestras x longitud)')\n",
    "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
    "print('input_train shape:', input_train.shape)\n",
    "print('input_test shape:', input_test.shape)\n",
    "\n",
    "# Crear el modelo RNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "print('Entrenando...')\n",
    "history = model.fit(input_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "print('Evaluando...')\n",
    "results = model.evaluate(input_test, y_test)\n",
    "print('Precisión en el conjunto de prueba:', results[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
